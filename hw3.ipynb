{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577079be-1c60-4313-afa6-3f1f06c88984",
   "metadata": {},
   "source": [
    "# Analysis of a Trained Model for Sentiment Classifation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fd6994-69db-4f87-a0d1-77b4185e5d01",
   "metadata": {},
   "source": [
    "## Trénink modelu\n",
    "\n",
    "Trénovánání modelu je zastaveno pomocí early stoping.\n",
    "\n",
    "| Epoch: 01 | Train Loss: 0.780 | Train Acc: 51.24% | Val. Loss: 0.672 | Val. Acc: 61.57% |\n",
    "\n",
    "| Epoch: 02 | Train Loss: 0.680 | Train Acc: 56.38% | Val. Loss: 0.633 | Val. Acc: 67.60% |\n",
    "\n",
    "| Epoch: 03 | Train Loss: 0.659 | Train Acc: 60.32% | Val. Loss: 0.603 | Val. Acc: 71.50% |\n",
    "\n",
    "| Epoch: 04 | Train Loss: 0.636 | Train Acc: 63.55% | Val. Loss: 0.572 | Val. Acc: 73.10% |\n",
    "\n",
    "| Epoch: 05 | Train Loss: 0.607 | Train Acc: 66.57% | Val. Loss: 0.525 | Val. Acc: 75.02% |\n",
    "\n",
    "| Epoch: 06 | Train Loss: 0.575 | Train Acc: 69.44% | Val. Loss: 0.491 | Val. Acc: 75.87% |\n",
    "\n",
    "| Epoch: 07 | Train Loss: 0.551 | Train Acc: 71.72% | Val. Loss: 0.468 | Val. Acc: 78.72% |\n",
    "\n",
    "| Epoch: 08 | Train Loss: 0.529 | Train Acc: 73.57% | Val. Loss: 0.452 | Val. Acc: 79.47% |\n",
    "\n",
    "| Epoch: 09 | Train Loss: 0.507 | Train Acc: 75.20% | Val. Loss: 0.427 | Val. Acc: 80.93% |\n",
    "\n",
    "| Epoch: 10 | Train Loss: 0.490 | Train Acc: 76.50% | Val. Loss: 0.413 | Val. Acc: 81.33% |\n",
    "\n",
    "| Epoch: 11 | Train Loss: 0.463 | Train Acc: 78.11% | Val. Loss: 0.407 | Val. Acc: 81.45% |\n",
    "\n",
    "| Epoch: 12 | Train Loss: 0.447 | Train Acc: 79.30% | Val. Loss: 0.393 | Val. Acc: 82.28% |\n",
    "\n",
    "| Epoch: 13 | Train Loss: 0.435 | Train Acc: 79.82% | Val. Loss: 0.383 | Val. Acc: 83.25% |\n",
    "\n",
    "| Epoch: 14 | Train Loss: 0.422 | Train Acc: 81.06% | Val. Loss: 0.379 | Val. Acc: 83.62% |\n",
    "\n",
    "| Epoch: 15 | Train Loss: 0.406 | Train Acc: 81.91% | Val. Loss: 0.374 | Val. Acc: 83.50% |\n",
    "\n",
    "| Epoch: 16 | Train Loss: 0.393 | Train Acc: 82.91% | Val. Loss: 0.371 | Val. Acc: 83.31% |\n",
    "\n",
    "| Epoch: 17 | Train Loss: 0.382 | Train Acc: 83.72% | Val. Loss: 0.369 | Val. Acc: 84.16% |\n",
    "\n",
    "| Epoch: 18 | Train Loss: 0.368 | Train Acc: 84.40% | Val. Loss: 0.364 | Val. Acc: 84.57% |\n",
    "\n",
    "| Epoch: 19 | Train Loss: 0.358 | Train Acc: 85.02% | Val. Loss: 0.363 | Val. Acc: 84.45% |\n",
    "\n",
    "| Epoch: 20 | Train Loss: 0.347 | Train Acc: 85.47% | Val. Loss: 0.358 | Val. Acc: 85.13% |\n",
    "\n",
    "| Epoch: 21 | Train Loss: 0.337 | Train Acc: 86.36% | Val. Loss: 0.359 | Val. Acc: 84.89% |\n",
    "\n",
    "| Epoch: 22 | Train Loss: 0.325 | Train Acc: 86.75% | Val. Loss: 0.355 | Val. Acc: 85.11% |\n",
    "\n",
    "| Epoch: 23 | Train Loss: 0.316 | Train Acc: 87.14% | Val. Loss: 0.356 | Val. Acc: 85.19% |\n",
    "\n",
    "| Epoch: 24 | Train Loss: 0.309 | Train Acc: 87.46% | Val. Loss: 0.363 | Val. Acc: 84.95% |\n",
    "\n",
    "| Epoch: 25 | Train Loss: 0.304 | Train Acc: 87.92% | Val. Loss: 0.362 | Val. Acc: 84.97% |\n",
    "\n",
    "| Epoch: 26 | Train Loss: 0.297 | Train Acc: 88.19% | Val. Loss: 0.365 | Val. Acc: 84.87% |\n",
    "\n",
    "| Epoch: 27 | Train Loss: 0.284 | Train Acc: 88.98% | Val. Loss: 0.360 | Val. Acc: 85.36% |\n",
    "\n",
    "| Epoch: 28 | Train Loss: 0.288 | Train Acc: 88.69% | Val. Loss: 0.366 | Val. Acc: 85.17% |\n",
    "\n",
    "| Epoch: 29 | Train Loss: 0.273 | Train Acc: 89.14% | Val. Loss: 0.366 | Val. Acc: 84.87% |\n",
    "\n",
    "| Epoch: 30 | Train Loss: 0.264 | Train Acc: 89.68% | Val. Loss: 0.370 | Val. Acc: 85.32% |\n",
    "\n",
    "| Epoch: 31 | Train Loss: 0.265 | Train Acc: 89.58% | Val. Loss: 0.367 | Val. Acc: 85.09% |\n",
    "\n",
    "| Epoch: 32 | Train Loss: 0.258 | Train Acc: 90.10% | Val. Loss: 0.365 | Val. Acc: 85.05% |\n",
    "\n",
    "| Epoch: 33 | Train Loss: 0.247 | Train Acc: 90.14% | Val. Loss: 0.369 | Val. Acc: 85.23% |\n",
    "\n",
    "Training finished, testing the model:\n",
    "| Test Loss: 0.369 | Test Acc: 84.30% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363e53f-3459-433e-a0c0-9b19ede79568",
   "metadata": {},
   "source": [
    "FILTER INDEX 0\n",
    "1.      worst   score:  4.42\n",
    "2.      wooden  score:  4.33\n",
    "3.      pointless       score:  4.26\n",
    "4.      supposed        score:  4.22\n",
    "5.      fails   score:  3.95\n",
    "6.      wasted  score:  3.94\n",
    "7.      dull    score:  3.81\n",
    "8.      boring  score:  3.66\n",
    "9.      remotely        score:  3.56\n",
    "10.     unfortunately   score:  3.54\n",
    "11.     ridiculous      score:  3.33\n",
    "12.     annoying        score:  3.33\n",
    "13.     stupid  score:  3.28\n",
    "14.     disappointment  score:  3.27\n",
    "15.     mess    score:  3.27\n",
    "\n",
    "FILTER INDEX 1\n",
    "1.      today   score:  5.49\n",
    "2.      powerful        score:  5.28\n",
    "3.      love    score:  5.17\n",
    "4.      superb  score:  4.46\n",
    "5.      ##rell  score:  3.93\n",
    "6.      brooklyn        score:  3.88\n",
    "7.      ##hul   score:  3.77\n",
    "8.      perfect score:  3.76\n",
    "9.      tragic  score:  3.44\n",
    "10.     shine   score:  3.38\n",
    "11.     fun     score:  3.35\n",
    "12.     princess        score:  3.31\n",
    "13.     deeper  score:  3.3\n",
    "14.     refreshing      score:  3.26\n",
    "15.     recommended     score:  3.22\n",
    "\n",
    "FILTER INDEX 2\n",
    "1.      worst   score:  5.14\n",
    "2.      wasted  score:  4.96\n",
    "3.      awful   score:  4.8\n",
    "4.      waste   score:  4.69\n",
    "5.      mess    score:  4.67\n",
    "6.      dull    score:  4.49\n",
    "7.      fails   score:  4.46\n",
    "8.      annoying        score:  4.4\n",
    "9.      supposed        score:  4.27\n",
    "10.     unfortunately   score:  4.23\n",
    "11.     ridiculous      score:  4.04\n",
    "12.     remotely        score:  4.01\n",
    "13.     boring  score:  3.81\n",
    "14.     terrible        score:  3.53\n",
    "15.     pointless       score:  3.53\n",
    "\n",
    "FILTER INDEX 3\n",
    "1.      excellent       score:  5.12\n",
    "2.      loved   score:  5.03\n",
    "3.      rare    score:  4.39\n",
    "4.      favorite        score:  4.05\n",
    "5.      finest  score:  4.05\n",
    "6.      princess        score:  3.21\n",
    "7.      brooklyn        score:  3.15\n",
    "8.      haunting        score:  3.12\n",
    "9.      perfect score:  3.05\n",
    "10.     unique  score:  3.03\n",
    "11.     refreshing      score:  2.97\n",
    "12.     delicious       score:  2.9\n",
    "13.     vulcan  score:  2.89\n",
    "14.     trey    score:  2.83\n",
    "15.     solid   score:  2.81\n",
    "\n",
    "FILTER INDEX 4\n",
    "1.      today   score:  4.55\n",
    "2.      fun     score:  3.65\n",
    "3.      powerful        score:  3.62\n",
    "4.      ##hul   score:  3.25\n",
    "5.      superb  score:  3.23\n",
    "6.      brooklyn        score:  3.16\n",
    "7.      wonderful       score:  3.14\n",
    "8.      perfect score:  3.07\n",
    "9.      tragic  score:  3.03\n",
    "10.     refreshing      score:  2.95\n",
    "11.     finest  score:  2.93\n",
    "12.     princess        score:  2.81\n",
    "13.     appreciated     score:  2.8\n",
    "14.     landmark        score:  2.69\n",
    "15.     flawless        score:  2.64\n",
    "\n",
    "FILTER INDEX 5\n",
    "1.      refreshing      score:  5.22\n",
    "2.      7       score:  4.41\n",
    "3.      brilliant       score:  4.35\n",
    "4.      wonderful       score:  4.33\n",
    "5.      fantastic       score:  4.32\n",
    "6.      brooklyn        score:  4.22\n",
    "7.      flawless        score:  4.13\n",
    "8.      enjoyable       score:  4.09\n",
    "9.      8       score:  4.0\n",
    "10.     atmosphere      score:  3.93\n",
    "11.     defines score:  3.79\n",
    "12.     amazing score:  3.75\n",
    "13.     rocked  score:  3.64\n",
    "14.     favorites       score:  3.53\n",
    "15.     precursor       score:  3.5\n",
    "\n",
    "FILTER INDEX 6\n",
    "1.      today   score:  5.65\n",
    "2.      powerful        score:  5.57\n",
    "3.      superb  score:  4.3\n",
    "4.      ##rell  score:  3.98\n",
    "5.      princess        score:  3.7\n",
    "6.      favorites       score:  3.67\n",
    "7.      ##hul   score:  3.57\n",
    "8.      brooklyn        score:  3.57\n",
    "9.      perfect score:  3.56\n",
    "10.     victoria        score:  3.52\n",
    "11.     touching        score:  3.44\n",
    "12.     fun     score:  3.44\n",
    "13.     finest  score:  3.4\n",
    "14.     amazing score:  3.36\n",
    "15.     best    score:  3.35\n",
    "\n",
    "FILTER INDEX 7\n",
    "1.      excellent       score:  4.79\n",
    "2.      loved   score:  4.54\n",
    "3.      favorite        score:  4.31\n",
    "4.      perfect score:  4.04\n",
    "5.      rare    score:  3.81\n",
    "6.      finest  score:  3.76\n",
    "7.      gem     score:  3.08\n",
    "8.      tragic  score:  3.08\n",
    "9.      touching        score:  3.05\n",
    "10.     stewart score:  2.9\n",
    "11.     unique  score:  2.87\n",
    "12.     solid   score:  2.78\n",
    "13.     defines score:  2.73\n",
    "14.     refreshing      score:  2.73\n",
    "15.     pei     score:  2.64\n",
    "\n",
    "FILTER INDEX 8\n",
    "1.      wonderful       score:  6.13\n",
    "2.      beautiful       score:  5.78\n",
    "3.      refreshing      score:  5.28\n",
    "4.      7       score:  5.2\n",
    "5.      enjoyed score:  5.02\n",
    "6.      brilliant       score:  4.98\n",
    "7.      favorites       score:  4.69\n",
    "8.      fantastic       score:  4.26\n",
    "9.      brooklyn        score:  4.25\n",
    "10.     sweet   score:  4.17\n",
    "11.     appreciated     score:  4.16\n",
    "12.     flawless        score:  4.02\n",
    "13.     ##rell  score:  4.01\n",
    "14.     perfectly       score:  3.99\n",
    "15.     superb  score:  3.92\n",
    "\n",
    "FILTER INDEX 9\n",
    "1.      beautiful       score:  5.45\n",
    "2.      wonderful       score:  5.32\n",
    "3.      refreshing      score:  4.65\n",
    "4.      enjoyed score:  4.42\n",
    "5.      perfectly       score:  4.03\n",
    "6.      flawless        score:  4.03\n",
    "7.      favorites       score:  3.92\n",
    "8.      superb  score:  3.89\n",
    "9.      appreciated     score:  3.65\n",
    "10.     ##pass  score:  3.46\n",
    "11.     sweet   score:  3.43\n",
    "12.     brilliant       score:  3.37\n",
    "13.     anton   score:  3.32\n",
    "14.     ##rell  score:  3.29\n",
    "15.     amazing score:  3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db784f-549a-40f7-bb8c-b07b570e2008",
   "metadata": {},
   "source": [
    "## Kvalitativní hodnocení\n",
    "\n",
    "V našich top slovech se nejvíce vyskytovala pozitivně a negativně zbarvená slova. Kde nejčastějším slovním druhem je zjevně\n",
    "přídavné jméno.\n",
    "\n",
    "Každý filtr zachycuje pozitivní a negativní slova zvlášť, kde filtry 1,3,4,5,6,7,8 a 9 zachycují pozitivní slova a filtry 0 a 2 zachycují negativní slova. V jednotlivých filtrech můžeme také vidět outliery, jako ve filtru 6 se slovy ##rell a ##hul. Nekteré filtry zachytily i číslovky. Příkladem takových filtrů jsou 8 a 5. Filtry 1,6,8 a 9 zachytily slovo s hastagem.\n",
    "\n",
    "Dále si všimněme, že slova nejlépe umístěna v daném filtru mají skore okolo hodnoty 4.4 až 6.2 a postupně skore daných slov klesá až na skore okolo 2.64 až 4.\n",
    "\n",
    "Filtry 6,4 a 1 slovo zachytily today, jako top 1 i přesto, že today není vysloveně pozitivní slovo.\n",
    "\n",
    "Některá slova byla zachycena ve více filtrech. (Nemyslím daný filtr, tam duplikáty nemáme).\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d918f-559d-4006-9643-6fe9b694ede4",
   "metadata": {},
   "source": [
    "## Analýza zachycených POS triggers\n",
    "Použil jsem data ze zadání, kde tento dataset slov zachytil 135 slov z 10*15 = 150 slov s výsledkem \n",
    "\n",
    "ADJ     70      51.85%\n",
    "\n",
    "VERB    28      20.74%\n",
    "\n",
    "NOUN    22      16.3%\n",
    "\n",
    "PROPN   8       5.93%\n",
    "\n",
    "ADV     4       2.96%\n",
    "\n",
    "NUM     3       2.22%\n",
    "\n",
    "Kde více než polovina zachycených slov je ADJ. Nebylo zachyceno 15 slov, které jsou:\n",
    "\n",
    "remotely\n",
    "\n",
    "rell\n",
    "\n",
    "hul\n",
    "\n",
    "remotely\n",
    "\n",
    "haunting\n",
    "\n",
    "vulcan\n",
    "\n",
    "trey\n",
    "\n",
    "hul\n",
    "\n",
    "precursor\n",
    "\n",
    "rell\n",
    "\n",
    "hul\n",
    "\n",
    "victoria\n",
    "\n",
    "pei\n",
    "\n",
    "rell\n",
    "\n",
    "rell\n",
    "\n",
    "Pokud dopočítáme POS těchto slov pomocí nltk knihovna, pak dostáváme:\n",
    "\n",
    "remotely         ADV\n",
    "\n",
    "rell             VERB\n",
    "\n",
    "hul              ADV\n",
    "\n",
    "remotely         ADV\n",
    "\n",
    "haunting         VERB\n",
    "\n",
    "vulcan           ADJ\n",
    "\n",
    "trey             NOUN\n",
    "\n",
    "hul              NOUN\n",
    "\n",
    "precursor        NOUN\n",
    "\n",
    "rell             NOUN\n",
    "\n",
    "hul              NOUN\n",
    "\n",
    "victoria         NOUN\n",
    "\n",
    "pei              NOUN\n",
    "\n",
    "rell             NOUN\n",
    "\n",
    "rell             NOUN\n",
    "\n",
    "Potom bychom dostali, po doplnění výsledky:\n",
    "\n",
    "ADJ     72\n",
    "\n",
    "VERB    30      \n",
    "\n",
    "NOUN    31      \n",
    "\n",
    "PROPN   8       \n",
    "\n",
    "ADV     6       \n",
    "\n",
    "NUM     3      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cafe88-d7de-4f17-9485-6a2d0bec9c53",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc012e3-cb18-4c4a-a165-40741b62eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from conllu import parse_incr\n",
    "import re\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "VOCAB_SIZE = 0\n",
    "\n",
    "class Splitter:\n",
    "    TAGS = [\"train\",\"dev\",\"test\"]\n",
    "    def __init__(self):\n",
    "        global VOCAB_SIZE\n",
    "        \n",
    "        self.data = defaultdict(DataLoader)\n",
    "    \n",
    "        imdb = load_dataset(\"imdb\")\n",
    "\n",
    "        split_train = imdb[\"train\"].train_test_split(0.2)\n",
    "        imdb[\"train\"] = split_train[\"train\"]\n",
    "        imdb[\"dev\"] = split_train[\"test\"]\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "        VOCAB_SIZE = len(self.tokenizer.get_vocab())\n",
    "\n",
    "        tokenized_imdb = imdb.map(partial(self.preprocess_function, tokenizer=self.tokenizer), batched=True, remove_columns=\"text\")\n",
    "\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "\n",
    "        for tag in self.TAGS:\n",
    "            self.data[tag] = DataLoader(tokenized_imdb[tag],shuffle=True, collate_fn=data_collator,batch_size=64)\n",
    "    \n",
    "    def preprocess_function(self,examples,tokenizer):\n",
    "        return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "class Training:\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def binary_accuracy(self,preds, y):\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float()\n",
    "\n",
    "        return correct.mean()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self,model, dataloader, criterion):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        steps = 0\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for batch in dataloader:\n",
    "            batch.to(device)\n",
    "\n",
    "            predictions = model(batch).squeeze(1)\n",
    "            loss = criterion(predictions, batch.labels.float())\n",
    "\n",
    "            acc = self.binary_accuracy(predictions, batch.labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            steps += 1\n",
    "\n",
    "        return epoch_loss / steps, epoch_acc / steps\n",
    "\n",
    "    def train_epoch(self,model, dataloader, optimizer, criterion):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        steps = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            batch.to(device)\n",
    "\n",
    "            predictions = model(batch).squeeze(1)\n",
    "            loss = criterion(predictions, batch.labels.float())\n",
    "            acc = self.binary_accuracy(predictions, batch.labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            steps += 1\n",
    "\n",
    "        return epoch_loss / steps, epoch_acc / steps\n",
    "\n",
    "    def training_loop(self,model,data,MAX_NOT_CHANGE = 10):\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        criterion = criterion.to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        train_dataloader = data[Splitter.TAGS[0]]\n",
    "        eval_dataloader = data[Splitter.TAGS[1]]\n",
    "        test_dataloader = data[Splitter.TAGS[2]]\n",
    "\n",
    "        best_valid_loss = float(\"inf\")\n",
    "        not_change = 0\n",
    "        epoch = 0\n",
    "\n",
    "        while True:\n",
    "            train_loss, train_acc = self.train_epoch(model, train_dataloader, optimizer, criterion)\n",
    "            valid_loss, valid_acc = self.evaluate(model, eval_dataloader, criterion)\n",
    "\n",
    "            print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "\n",
    "            if not_change == MAX_NOT_CHANGE:\n",
    "                break\n",
    "\n",
    "            if best_valid_loss <= valid_loss:\n",
    "                not_change += 1\n",
    "            \n",
    "            elif best_valid_loss > valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                not_change = 0\n",
    "                torch.save(model.state_dict(), \"hw3_model.pt\")\n",
    "            \n",
    "            epoch += 1\n",
    "    \n",
    "        print()\n",
    "        print(\"Training finished, testing the model:\")\n",
    "        model.load_state_dict(torch.load(\"hw3_model.pt\"))\n",
    "        test_loss, test_acc = self.evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "        print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')\n",
    "\n",
    "\n",
    "class Convolutional(nn.Module):\n",
    "    def __init__(self, embedding_dim, kernel_sizes, filters, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: create all necessary layers\n",
    "        # hint: if you want to use a list of modules, you need to use `nn.ModuleList`\n",
    "\n",
    "        self.embeddings = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
    "        self.conv = nn.ModuleList([nn.Sequential(nn.Dropout(0.6),nn.Conv1d(embedding_dim, filters, k),nn.ReLU())for k in kernel_sizes])\n",
    "        self.linear = nn.Sequential(nn.Dropout(0.6),nn.Linear(len(kernel_sizes) * filters, 1))\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        # TODO: apply the layers on data x\n",
    "        # hint: max-pooling is just applying maximum over the correct axis, do not use max-pooling operators from PyTorch\n",
    "\n",
    "        embedded = self.embeddings(batch.input_ids).transpose(1, 2)\n",
    "        max_pooled = torch.cat([conv(embedded).max(dim=2).values for conv in self.conv], dim=1)\n",
    "\n",
    "        return self.linear(max_pooled)\n",
    "\n",
    "class Solution:\n",
    "    def solution(model,data:Splitter,TOPK_NUM = 15,UD_ENGLISH_EWT = [\"UD_English-EWT/en_ewt-ud-train.conllu\",\"UD_English-EWT/en_ewt-ud-dev.conllu\",\"UD_English-EWT/en_ewt-ud-test.conllu\"]):\n",
    "        # TODO: Using the input word embeddings (you will likely find them in model.embeddings.weight)\n",
    "        word_embeddings = model.embeddings.weight\n",
    "        \n",
    "        # TODO: the convolutional filter weights (likely in model.conv[0][1].weight)\n",
    "        convolutional_filter_weights = model.conv[0][1].weight\n",
    "        \n",
    "        # TODO: you wil have to transpose the weights correctly\n",
    "        weights_filter_squeeze = convolutional_filter_weights.squeeze(2)\n",
    "        weights_filter_transpons = weights_filter_squeeze.transpose(0, 1)\n",
    "\n",
    "        # TODO: find the tokens that lead to the highest filter responses. The response is computed as a dot product of the respective word embeddings and vectors from the weight matrices\n",
    "        response = torch.matmul(word_embeddings,weights_filter_transpons)\n",
    "\n",
    "        # TODO: then you can find the best-scoring ones using topk function, think of setting the correct dimension). \n",
    "        # For simplicity, you can only work with kernels of size 1 but feel free to consider longer spans too. \n",
    "        results = response.topk(k=TOPK_NUM, dim=0)\n",
    "\n",
    "        # TODO: Look at the results and qualitatively assess what words appear among the best-scoring ones\n",
    "        # Method tokenizer.convert_ids_to_tokes might be useful to convert the indices back to tokens.\n",
    "        \n",
    "        top_tokens = []\n",
    "\n",
    "        for filter_idx in range(response.shape[1]):\n",
    "            idx_filter_scores = [result[filter_idx].item() for result in results[0]]\n",
    "            idx_filter_tokens = [result[filter_idx].item() for result in results[1]]\n",
    "            sorted_tokens = data.tokenizer.convert_ids_to_tokens(idx_filter_tokens)\n",
    "\n",
    "            print(f\"FILTER INDEX {filter_idx}\")\n",
    "            for position, (token, score) in enumerate(zip(sorted_tokens, idx_filter_scores)):\n",
    "               print(f\"{position + 1}.\\t{token}\\tscore:\\t{round(score,2)}\")\n",
    "               top_tokens.append(token)\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        # TODO :For each word, only consider the most frequent POS tag. (You can get the most frequent POS tags, e.g., from the English Web Treebank.)\n",
    "        most_frequent_POS_word = defaultdict(Counter)\n",
    "\n",
    "        for file_Path in UD_ENGLISH_EWT:\n",
    "\n",
    "            with open(file_Path, \"r\", encoding=\"utf-8\") as file:\n",
    "\n",
    "                for tokens in parse_incr(file):\n",
    "                    for token in tokens:\n",
    "                        word = token[\"form\"]\n",
    "                        pos = token[\"upos\"]\n",
    "                        word = word.lower()\n",
    "                        most_frequent_POS_word[word][pos] += 1\n",
    "\n",
    "        poses = Counter()\n",
    "        count_all = 0\n",
    "\n",
    "        for token in top_tokens:\n",
    "            token = token.lower()\n",
    "            token = re.sub(r'[^a-z0-9]', '', token)\n",
    "\n",
    "            if token in most_frequent_POS_word:\n",
    "                most_frequent_pos_tag = most_frequent_POS_word[token].most_common(1)[0][0]\n",
    "                poses[ most_frequent_pos_tag] += 1\n",
    "                count_all += 1\n",
    "            else:\n",
    "                print(token)\n",
    "\n",
    "        # TODO:Analyze what POS triggers the convolutional filters the most: compute a statistic how often different POS appear among the best scoring words. For each word, only consider the most frequent POS tag.\n",
    "        for pos, count in poses.most_common():\n",
    "            print(f\"{pos}\\t{count}\\t{round(count / count_all * 100, 2)}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ------------------------------------------------------- This was taken from your lab nootebook >\n",
    "    dataset = Splitter()\n",
    "    model = Convolutional(200, [1,2,3,4,5], 10, 1)\n",
    "    model = model.to(device)\n",
    "    Training().training_loop(model,dataset.data)  # upraven, jelikož používáme early stopping\n",
    "\n",
    "    # ------------------------------------------------------- solution of task is in this method >\n",
    "    Solution.solution(model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
